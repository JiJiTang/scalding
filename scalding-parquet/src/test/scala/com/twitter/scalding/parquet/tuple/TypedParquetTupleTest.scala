package com.twitter.scalding.parquet.tuple

import java.io.File

import cascading.tuple.{Fields, TupleEntry, Tuple}
import com.twitter.scalding.typed.TypedPipe
import com.twitter.scalding.{TupleSetter, TupleConverter, Job, Args, Mode, TypedCsv}
import scala.io.{Source => ScalaSource}

import org.apache.hadoop
import org.scalatest.{Matchers, WordSpec}

class TypedParquetTupleTest extends WordSpec with Matchers {
  "TypedParquetTuple" should {

    "read and write correctly" in {

      val values = Seq(SampleClass("A", 1, 4.0D), SampleClass("B", 2, 3.0D), SampleClass("A", 2, 5.0D))

      //write values to typed parquet sink
      val tempParquet = createTempDir("parquet_tuple_test_parquet_")
      val writeJobArgs = buildJobArgs(Array("--output", tempParquet, "--hdfs"))
      val writeSuccess = new WriteToTypedParquetTupleJob(values, writeJobArgs).run
      writeSuccess shouldEqual true

      //read from the written parquet tuple and write the values read into a csv file
      val tempCsv = createTempDir("parquet_tuple_test_csv_")
      val readJobArgs = buildJobArgs(Array("--input", tempParquet, "--output", tempCsv, "--hdfs"))
      val readSuccess = new ReadFromTypedParquetTupleJob(readJobArgs).run
      readSuccess shouldEqual true

      //check data correctness
      val csv = ScalaSource.fromFile(new java.io.File(tempCsv, "part-00000"))
      csv.getLines().toList shouldEqual Seq("A,1,4.0", "B,2,3.0", "A,2,5.0")

      //clean temporary files generated by the jobs
      deletePath(tempCsv)
      deletePath(tempParquet)
    }
  }

  def buildJobArgs(argArray: Array[String]): Args = {
    val tool = new com.twitter.scalding.Tool
    tool.setConf(new hadoop.mapred.JobConf)
    val args = tool.parseModeArgs(argArray)
    Mode.putMode(args._1, args._2)
  }

  def createTempDir(prefix: String): String = {
    java.nio.file.Files.createTempDirectory(prefix).toAbsolutePath.toString
  }

  def deletePath(path: String) = {
    val dir = new File(path)
    for {
      files <- Option(dir.listFiles)
      file <- files
    } file.delete()
    dir.delete()
  }
}

case class SampleClass(stringValue: String, intValue: Int, doubleValue: Double)

/**
 * Test job write a sequence of sample class values into a typed parquet tuple.
 * To test typed parquet tuple can be used as sink
 */
class WriteToTypedParquetTupleJob(values: Seq[SampleClass], args: Args) extends Job(args) {

  import SampleClassDescriptor._

  val outputPath = args.required("output")

  val parquetTuple = TypedParquetTuple[SampleClass](Seq(outputPath), sampleClassFields, sampleClassParquetSchema)
  TypedPipe.from(values).write(parquetTuple)
}

/**
 * Test job read from a typed parquet tuple and write the mapped value into a typed csv sink
 * To test typed parquet tuple can bse used as source and read data correctly
 */
class ReadFromTypedParquetTupleJob(args: Args) extends Job(args) {

  import SampleClassDescriptor._

  val inputPath = args.required("input")
  val outputPath = args.required("output")

  val parquetTuple = TypedParquetTuple[SampleClass](Seq(inputPath), sampleClassFields)

  TypedPipe.from(parquetTuple).map {
    case SampleClass(string, int, double) => (string, int, double)
  }.write(TypedCsv[(String, Int, Double)](outputPath))
}

/**
 * Helper class with tuple related setter and converter +
 * parquet schema using parquet schema generation macro
 */
object SampleClassDescriptor {

  import com.twitter.scalding.parquet.tuple.macros.Macros._

  implicit val valueTupleSetter = new TupleSetter[SampleClass] {
    override def apply(value: SampleClass): Tuple = {
      val tuple = new Tuple()
      tuple.addString(value.stringValue)
      tuple.addInteger(value.intValue)
      tuple.addDouble(value.doubleValue)
      tuple
    }

    override def arity: Int = 3
  }

  implicit val valueTupleConverter = new TupleConverter[SampleClass] {
    override def apply(te: TupleEntry): SampleClass = {
      val stringValue = te.getString("stringValue")
      val intValue = te.getInteger("intValue")
      val doubleValue = te.getDouble("doubleValue")
      SampleClass(stringValue, intValue, doubleValue)
    }

    override def arity: Int = 3
  }

  val sampleClassFields: Fields = new Fields("stringValue", "intValue", "doubleValue")
  val sampleClassParquetSchema = caseClassParquetSchema[SampleClass]
}
